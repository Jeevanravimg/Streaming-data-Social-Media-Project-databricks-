{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7979135c-f7d1-4bae-a06a-3353e6f79146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Notebook Documentation\n",
    "\n",
    "- Created a dataset for streaming using the `faker` library to generate synthetic data.\n",
    "- Saved the generated data as JSON files in a Databricks volume.\n",
    "- Read and previewed the generated files using Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea1d1db-feaf-49dc-bf05-065c576350ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker\n",
    "from faker import Faker\n",
    "import json, time, random, os, threading\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Base directory for storing generated JSON files\n",
    "base_dir = \"/Volumes/kusha_solutions/jeevan_streaming/my_volume/Raw_JSON_Files/\"\n",
    "\n",
    "# Create subdirectories for each data type if they don't exist\n",
    "dirs = [\"users\", \"posts\", \"comments\", \"likes\"]\n",
    "for d in dirs:\n",
    "    os.makedirs(os.path.join(base_dir, d), exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# Generator Functions\n",
    "# =============================\n",
    "\n",
    "def generate_users():\n",
    "    \"\"\"\n",
    "    Continuously generates batches of fake user data and writes them to JSON files.\n",
    "    Each batch contains 20 users with unique IDs and random attributes.\n",
    "    \"\"\"\n",
    "    user_id = 1\n",
    "    output_dir = os.path.join(base_dir, \"users\")\n",
    "    while True:\n",
    "        data = []\n",
    "        for _ in range(20):\n",
    "            user = {\n",
    "                \"user_id\": user_id,\n",
    "                \"name\": fake.name(),\n",
    "                \"username\": fake.user_name(),\n",
    "                \"email\": fake.email(),\n",
    "                \"location\": fake.city(),\n",
    "                \"created_at\": fake.iso8601()\n",
    "            }\n",
    "            data.append(user)\n",
    "            user_id += 1\n",
    "\n",
    "        filename = f\"{output_dir}/users_{int(time.time())}.json\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "        print(f\"âœ… Generated: {filename}\")\n",
    "        time.sleep(10)\n",
    "\n",
    "def generate_posts():\n",
    "    \"\"\"\n",
    "    Continuously generates batches of fake post data and writes them to JSON files.\n",
    "    Each batch contains 20 posts with random user IDs, content, and like counts.\n",
    "    \"\"\"\n",
    "    post_id = 1\n",
    "    output_dir = os.path.join(base_dir, \"posts\")\n",
    "    while True:\n",
    "        data = []\n",
    "        for _ in range(20):\n",
    "            post = {\n",
    "                \"post_id\": post_id,\n",
    "                \"user_id\": random.randint(1, 1000),\n",
    "                \"content\": fake.sentence(nb_words=12),\n",
    "                \"likes\": random.randint(0, 1000000),\n",
    "                \"created_at\": fake.iso8601()\n",
    "            }\n",
    "            data.append(post)\n",
    "            post_id += 1\n",
    "\n",
    "        filename = f\"{output_dir}/posts_{int(time.time())}.json\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "        print(f\"âœ… Generated: {filename}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "def generate_comments():\n",
    "    \"\"\"\n",
    "    Continuously generates batches of fake comment data and writes them to JSON files.\n",
    "    Each batch contains 50 comments linked to random posts and users.\n",
    "    \"\"\"\n",
    "    comment_id = 1\n",
    "    output_dir = os.path.join(base_dir, \"comments\")\n",
    "    while True:\n",
    "        data = []\n",
    "        for _ in range(50):\n",
    "            comment = {\n",
    "                \"comment_id\": comment_id,\n",
    "                \"post_id\": random.randint(1, 1000),\n",
    "                \"user_id\": random.randint(1, 1000),\n",
    "                \"comment\": fake.sentence(nb_words=10),\n",
    "                \"created_at\": fake.iso8601()\n",
    "            }\n",
    "            data.append(comment)\n",
    "            comment_id += 1\n",
    "\n",
    "        filename = f\"{output_dir}/comments_{int(time.time())}.json\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "        print(f\"âœ… Generated: {filename}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "def generate_likes():\n",
    "    \"\"\"\n",
    "    Continuously generates batches of fake like/reaction data and writes them to JSON files.\n",
    "    Each batch contains 50 likes with random reactions for posts by users.\n",
    "    \"\"\"\n",
    "    like_id = 1\n",
    "    output_dir = os.path.join(base_dir, \"likes\")\n",
    "    while True:\n",
    "        data = []\n",
    "        for _ in range(50):\n",
    "            like = {\n",
    "                \"like_id\": like_id,\n",
    "                \"user_id\": random.randint(1, 1000),\n",
    "                \"post_id\": random.randint(1, 1000),\n",
    "                \"reaction\": random.choice([\"like\", \"love\", \"haha\", \"angry\"]),\n",
    "                \"created_at\": fake.iso8601()\n",
    "            }\n",
    "            data.append(like)\n",
    "            like_id += 1\n",
    "\n",
    "        filename = f\"{output_dir}/likes_{int(time.time())}.json\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "        print(f\"âœ… Generated: {filename}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "# =============================\n",
    "# Run all generators in parallel\n",
    "# =============================\n",
    "\n",
    "# Start each generator function in its own thread for parallel data generation\n",
    "threads = []\n",
    "for func in [generate_users, generate_posts, generate_comments, generate_likes]:\n",
    "    t = threading.Thread(target=func, daemon=True)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "print(\"ðŸš€ Data generation started for users, posts, comments, and likes â€” running continuously!\")\n",
    "\n",
    "# Keep the main thread alive so that generator threads keep running\n",
    "while True:\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a36b94-187d-4f08-bae4-7ee3a23dd993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiLine\", True).format(\"json\").load(\n",
    "    \"/Volumes/kusha_solutions/jeevan_streaming/my_volume/Raw_JSON_Files/users/\"\n",
    ").cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a694cec-98ea-482e-9a79-981abc18d772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiLine\", True).format(\"json\").load(\n",
    "    \"/Volumes/kusha_solutions/jeevan_streaming/my_volume/Raw_JSON_Files/posts/\"\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e9f6af-c07d-4e49-824f-17cfc40ea938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiLine\", True).format(\"json\").load(\n",
    "    \"/Volumes/kusha_solutions/jeevan_streaming/my_volume/Raw_JSON_Files/comments/\"\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc2675c7-fd22-4199-b894-9bc896cebe8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiLine\", True).format(\"json\").load(\n",
    "    \"/Volumes/kusha_solutions/jeevan_streaming/my_volume/Raw_JSON_Files/likes/\"\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9156ae1d-2f4c-446b-9d16-d06f3bb35868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"kusha_solutions.jeevan_streaming.gold_user_engagement_summary\")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Generation_JSON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
